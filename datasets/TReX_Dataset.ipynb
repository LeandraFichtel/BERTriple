{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of TReX dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-06b7316909f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mentity_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/wikidata/latest-truthy.nt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build a rdf dictionary from wikidata truthy\n",
    "import json\n",
    "\n",
    "entity_dictionary = {}\n",
    "with open(\"/data/wikidata/latest-truthy.nt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"rdf-schema#label\" in line and \"@en \" in line:\n",
    "            try:\n",
    "                s,p ,o = line.split(\"> \")\n",
    "                s = s.replace(\"<\", \"\")\n",
    "                o = o.replace(\"@en\", \"\")\n",
    "                start = o.index( \"\\\"\" ) + 1\n",
    "                end = o.index( \"\\\"\", start )\n",
    "                o = o[start:end]\n",
    "                s = s.encode('utf-8').decode('unicode-escape')\n",
    "                o = o.encode('utf-8').decode('unicode-escape')\n",
    "                    \n",
    "                s = s.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                o = o.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                \n",
    "                entity_dictionary[s] = o\n",
    "            except ValueError:\n",
    "                continue\n",
    "#with open('data/data/entity_dictionary', 'w') as fp:\n",
    "#    json.dump(entity_dictionary, fp, indent=4, sort_keys=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "\n",
    "def remove_duplicates(prop_triples):\n",
    "    for p_qid in prop_triples:\n",
    "        triples = prop_triples[p_qid]\n",
    "        seen = set()\n",
    "        new_l = []\n",
    "        for d in triples:\n",
    "            t = tuple(d.items())\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                new_l.append(d)\n",
    "        prop_triples[p_qid] = new_l\n",
    "    return prop_triples\n",
    "\n",
    "def substract_triples(original_prop_triples, substract_prop_triples):\n",
    "    for p_qid in original_prop_triples:\n",
    "        original_triples = original_prop_triples[p_qid]\n",
    "        substract_triples = substract_prop_triples[p_qid]\n",
    "        \n",
    "        to_be_removed = set()\n",
    "        for d in substract_triples:\n",
    "            to_be_removed.add(tuple(d.items()))\n",
    "\n",
    "        new_l = []\n",
    "        for d in original_triples:\n",
    "            t = tuple(d.items())\n",
    "            if t not in to_be_removed:\n",
    "                new_l.append(d)\n",
    "        original_prop_triples[p_qid] = new_l\n",
    "    return original_prop_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no label found in wikidata:( Q3292203 interchange\n",
      "no label found wikidata :( Q7579156 Spoon sweets\n",
      "no label found wikidata :( Q412609 uridine 5'-triphosphoric acid\n",
      "no label found wikidata :( Q7259490 pan loaf\n",
      "no label found wikidata :( Q421647 Triuranium octoxide\n",
      "no label found wikidata :( Q17004641 Episcopal lineage\n",
      "no label found wikidata :( Q1323442 scorched rice\n",
      "no label found wikidata :( Q7233414 Posset\n",
      "no label found wikidata :( Q423223 L-lyxose\n",
      "no label found wikidata :( Q28775 nicotinamide adenine dinucleotide\n",
      "no label found wikidata :( Q28775 nicotinamide adenine dinucleotide\n",
      "no label found wikidata :( Q7233414 Posset\n",
      "Added 964 triples of property P527.\n",
      "no label found wikidata :( Q18913178 Panagiotis Lafazanis\n",
      "Added 976 triples of property P103.\n",
      "no label found wikidata :( Q422281 Cholecystokinin\n",
      "no label found wikidata :( Q1024108 COPI\n",
      "no label found wikidata :( Q12745086 Inhibitor of apoptosis domain\n",
      "no label found wikidata :( Q5324056 ERM protein family\n",
      "no label found wikidata :( Q4489396 management accounting\n",
      "no label found in wikidata:( Q426745 hull\n",
      "no label found in wikidata:( Q504802 intersection\n",
      "no label found wikidata :( Q5323037 EGF-like domain\n",
      "no label found wikidata :( Q7936970 vitamin K deficiency\n",
      "no label found in wikidata:( Q79746 candle\n",
      "no label found wikidata :( Q190199 growth hormone\n",
      "no label found wikidata :( Q4446304 Transforming growth factor beta superfamily\n",
      "no label found wikidata :( Q2915563 cultural feminism\n",
      "no label found wikidata :( Q411812 RecA\n",
      "Added 950 triples of property P279.\n",
      "no label found wikidata :( Q7727443 The Consumer Goods\n",
      "no label found wikidata :( Q6888754 Modern Love Records\n",
      "Added 934 triples of property P740.\n",
      "Added 949 triples of property P1303.\n",
      "no label found wikidata :( Q7757700 The Polly Bergen Show\n",
      "no label found wikidata :( Q7741510 The Imogene Coca Show\n",
      "no label found wikidata :( Q5136332 Club Oasis\n",
      "Added 878 triples of property P449.\n",
      "Added 473 triples of property P140.\n",
      "no label found wikidata :( Q189842 Final Fantasy\n",
      "no label found wikidata :( Q3812948 Kardinia Park\n",
      "Added 685 triples of property P127.\n",
      "no label found wikidata :( Q7427317 SaÃºl Levi Morteira\n",
      "Added 952 triples of property P20.\n",
      "no label found wikidata :( Q5380327 Enterprise Island\n",
      "no label found wikidata :( Q3761669 Reeves Glacier\n",
      "no label found wikidata :( Q7790052 Thomas Glacier\n",
      "no label found wikidata :( Q7927118 Victoria Upper Glacier\n",
      "no label found wikidata :( Q6794459 Mawson Glacier\n",
      "Added 970 triples of property P30.\n",
      "no label found wikidata :( Q17078828 Nekkonda\n",
      "Added 921 triples of property P31.\n",
      "no label found wikidata :( Q1539426 burgomaster\n",
      "Added 644 triples of property P138.\n",
      "Added 954 triples of property P937.\n",
      "Added 995 triples of property P190.\n",
      "no label found wikidata :( Q4863695 On Our Own Land\n",
      "Added 855 triples of property P364.\n",
      "Added 701 triples of property P1001.\n",
      "Added 892 triples of property P39.\n",
      "Added 958 triples of property P106.\n",
      "Added 234 triples of property P1376.\n",
      "Added 703 triples of property P36.\n",
      "no label found wikidata :( Q1156625 Andrea Orlandi\n",
      "no label found wikidata :( Q3525195 Thomas \"Hollywood\" Henderson\n",
      "Added 950 triples of property P413.\n",
      "Added 881 triples of property P131.\n",
      "no label found wikidata :( Q6203279 Jithan Ramesh\n",
      "no label found wikidata :( Q367143 Giovanni Juan Ignazio Molina\n",
      "Added 964 triples of property P27.\n",
      "no label found wikidata :( Q4255100 Latvian name\n",
      "no label found wikidata :( Q16255398 Superinteressante\n",
      "no label found wikidata :( Q8066734 Zarez\n",
      "Added 874 triples of property P407.\n",
      "no label found wikidata :( Q189842 Final Fantasy\n",
      "Added 591 triples of property P178.\n",
      "no label found wikidata :( Q1379239 Laurentius Roberg\n",
      "Added 695 triples of property P101.\n",
      "Added 969 triples of property P1412.\n",
      "Added 922 triples of property P47.\n",
      "no label found wikidata :( Q2056555 Laurdine \"Pat\" Patrick\n",
      "Added 930 triples of property P136.\n",
      "Added 996 triples of property P530.\n",
      "Added 966 triples of property P37.\n",
      "no label found wikidata :( Q6967257 Nasr-1\n",
      "no label found wikidata :( Q1424288 Honda CB450\n",
      "Added 980 triples of property P176.\n",
      "Added 429 triples of property P264.\n",
      "no label found wikidata :( Q64347 Cunter\n",
      "no label found wikidata :( Q4709134 Albanian Land Force\n",
      "no label found wikidata :( Q5028250 Campeche Bank\n",
      "no label found wikidata :( Q4396 All-Ukrainian Union \"Svoboda\"\n",
      "Added 926 triples of property P17.\n",
      "no label found in wikidata:( Q79746 candle\n",
      "no label found wikidata :( Q2725442 normative ethics\n",
      "no label found wikidata :( Q190057 catabolism\n",
      "no label found in wikidata:( Q2455704 subfamily\n",
      "no label found wikidata :( Q185057 telomere\n",
      "no label found wikidata :( Q3630470 consciousness raising\n",
      "no label found wikidata :( Q239606 speech act\n",
      "no label found wikidata :( Q15922520 couplet\n",
      "no label found wikidata :( Q5422636 side dishes\n",
      "no label found wikidata :( Q15181341 Antarasu\n",
      "Added 922 triples of property P361.\n",
      "no label found wikidata :( Q8068538 Zehra Sheerazi\n",
      "Added 943 triples of property P19.\n",
      "no label found wikidata :( Q15980792 Majlis-e-Ahrar-ul-Islam\n",
      "no label found wikidata :( Q7850772 Tube Investments of India Limited\n",
      "no label found wikidata :( Q6888754 Modern Love Records\n",
      "Added 964 triples of property P159.\n",
      "no label found wikidata :( Q7579156 Spoon sweets\n",
      "Added 908 triples of property P495.\n",
      "Added 383 triples of property P108.\n",
      "Added 225 triples of property P463.\n",
      "no label found wikidata :( Q7865811 USA Film Festival\n",
      "no label found wikidata :( Q4881051 Beidha\n",
      "no label found wikidata :( Q17518425 Wolvendaal Church\n",
      "no label found wikidata :( Q18207528 Great Ardra\n",
      "Added 955 triples of property P276.\n",
      "Lengths before removal 33961.\n",
      "Lengths after removal 33939.\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "trex_folder = \"/home/kalo/conferences/akbc2021/data/data/TREx/\"\n",
    "entity_dictionary_file = \"/home/kalo/conferences/akbc2021/data/data/entity_dictionary\"\n",
    "\n",
    "common_vocab_file = \"/home/kalo/conferences/akbc2021/common_vocab_cased.txt\"\n",
    "\n",
    "#trex_triple_file = \"/home/kalo/conferences/akbc2021/data/\"+\"trex_test.json\"\n",
    "trex_triple_file = \"/data/fichtel/BERTriple/test_datasets/\"+\"LAMA_trex_test.json\"\n",
    "entity_dictionary_trex_label_file = \"/data/fichtel/BERTriple/entity2label_trexlabel.json\"\n",
    "trex_test = {}\n",
    "\n",
    "\n",
    "def get_label(uri,entity_dictionary):\n",
    "    return entity_dictionary[uri] \n",
    "\n",
    "def load_trex_file(trex_data, entity_dictionary, common_vocab):\n",
    "    global trex_test\n",
    "    counter = 0\n",
    "    for triple_dict in trex_data:\n",
    "        s_qid = triple_dict[\"sub_uri\"]\n",
    "        p_qid = triple_dict[\"predicate_id\"]\n",
    "        o_qid = triple_dict[\"obj_uri\"]\n",
    "        s_label_trex = triple_dict[\"sub_label\"]\n",
    "        o_label_trex = triple_dict[\"obj_label\"]\n",
    "        if s_qid in entity_dictionary:\n",
    "            s_label = entity_dictionary[s_qid]\n",
    "            #change s_label to TREX label to have the same labels of testset and trainset\n",
    "            if s_label_trex != s_label:\n",
    "                entity_dictionary[s_qid] = s_label_trex\n",
    "                \n",
    "            if o_qid in entity_dictionary:\n",
    "                o_label = entity_dictionary[o_qid]\n",
    "                #change o_label to TREX label to have the same labels of testset and trainset\n",
    "                if o_label_trex != o_label:\n",
    "                    entity_dictionary[o_qid] = o_label_trex\n",
    "\n",
    "                #add tripel with current labels if the o_label is in common_vocab to have obj_queries\n",
    "                s_label = entity_dictionary[s_qid]\n",
    "                o_label = entity_dictionary[o_qid]\n",
    "                if o_label in common_vocab:\n",
    "                    triple = {\"subj\": s_label, \"prop\": p_qid, \"obj\": o_label}\n",
    "                    counter += 1\n",
    "                    if p_qid not in trex_test:\n",
    "                        trex_test[p_qid] = [triple]\n",
    "                    else:\n",
    "                        trex_test[p_qid].append(triple)\n",
    "            else:\n",
    "                print(\"no label found in wikidata:(\", o_qid, o_label_trex)\n",
    "        else:\n",
    "            print(\"no label found wikidata :(\", s_qid, s_label_trex)\n",
    "    print(\"Added {} triples of property {}.\".format(counter, p_qid))\n",
    "\n",
    "with open(entity_dictionary_file, \"r\") as f:\n",
    "    entity_dictionary = json.load(f)\n",
    "    with open(common_vocab_file, \"r\") as f:\n",
    "        common_vocab = set()\n",
    "        for line in f:\n",
    "            common_vocab.add(line.strip())\n",
    "        for file in os.listdir(trex_folder):\n",
    "            if file != \"wikidata_training.json\":\n",
    "                with open(trex_folder+file, \"r\") as f:\n",
    "                    trex_data = []\n",
    "                    for line in f.readlines():\n",
    "                        trex_data.append(json.loads(line))\n",
    "                    load_trex_file(trex_data, entity_dictionary, common_vocab)\n",
    "        \n",
    "        count_all_triples = 0\n",
    "        for p_qid in trex_test:\n",
    "            for triple in trex_test[p_qid]:\n",
    "                count_all_triples = count_all_triples + 1\n",
    "        print(\"Lengths before removal {}.\".format(count_all_triples))\n",
    "              \n",
    "        #remove duplicates of triples\n",
    "        trex_test = remove_duplicates(trex_test)\n",
    "        count_all_triples = 0\n",
    "        for p_qid in trex_test:\n",
    "            for triple in trex_test[p_qid]:\n",
    "                count_all_triples = count_all_triples + 1\n",
    "        print(\"Lengths after removal {}.\".format(count_all_triples))\n",
    "        \n",
    "        #save the test dataset of LAMA\n",
    "        with open(trex_triple_file, \"w\") as f:\n",
    "            json.dump(trex_test,f, indent=4, sort_keys=True)\n",
    "\n",
    "        #save the dictionary where the wikidata rdf labels are replaced with the trex labels when they were different\n",
    "        with open(entity_dictionary_trex_label_file, 'w') as fp:\n",
    "            json.dump(entity_dictionary, fp, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Wikidata training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths before substraction test from training data 106.\n",
      "Lengths after substraction test from training data 106.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "training_data = {}\n",
    "training_data[\"subj_queries\"] = {}\n",
    "training_data[\"obj_queries\"] = {}\n",
    "properties = {\"P1001\", \"P106\", \"P1303\", \"P1376\", \"P1412\", \"P178\", \"P19\", \"P276\", \"P30\", \"P364\", \"P39\", \"P449\", \"P495\", \"P740\", \"P101\", \"P108\", \"P131\", \"P138\", \"P159\", \"P17\", \"P20\", \"P279\", \"P31\", \"P36\", \"P407\", \"P463\", \"P527\", \"P937\", \"P103\", \"P127\", \"P136\", \"P140\", \"P176\", \"P190\", \"P264\", \"P27\", \"P361\", \"P37\", \"P413\", \"P47\", \"P530\"}\n",
    "#wikidata_training_path = \"/home/kalo/conferences/akbc2021/data/data/TREx/wikidata_training.json\"\n",
    "common_vocab_file = \"/home/kalo/conferences/akbc2021/common_vocab_cased.txt\"\n",
    "wikidata_training_path = \"/data/fichtel/BERTriple/training_datasets/wikidata41.json\"\n",
    "entity_dictionary_trex_label_file = \"/data/fichtel/BERTriple/entity2label_trexlabel.json\"\n",
    "trex_triple_file = \"/data/fichtel/BERTriple/test_datasets/\"+\"LAMA_trex_test.json\"\n",
    "\n",
    "with open(entity_dictionary_trex_label_file, \"r\") as f:\n",
    "    entity_dictionary_trex_label = json.load(f)\n",
    "    with open(common_vocab_file, \"r\") as f:\n",
    "        common_vocab = set()\n",
    "        for line in f:\n",
    "            common_vocab.add(line.strip())\n",
    "        with open(\"/data/wikidata/latest-truthy.nt\", \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    s,p,o = line.split(\"> <\")\n",
    "                    if \"wikidata.org/entity\" in o and \"wikidata.org/entity\" in s:\n",
    "\n",
    "                        s_qid = s.replace(\"<\", \"\").replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                        o_qid = o.replace(\"> .\\n\",\"\").replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                        p_qid = p.replace(\"http://www.wikidata.org/prop/direct/\",\"\").replace(\">\",\"\")\n",
    "                        if p_qid in properties:\n",
    "                            if s_qid in entity_dictionary_trex_label and o_qid in entity_dictionary_trex_label:\n",
    "                                s_label = entity_dictionary_trex_label[s_qid]\n",
    "                                o_label = entity_dictionary_trex_label[o_qid]\n",
    "                            \n",
    "                            if o_label in common_vocab:\n",
    "                                triple = {\"subj\": s_label, \"prop\": p_qid, \"obj\": o_label}\n",
    "                                if p_qid not in training_data[\"obj_queries\"]:\n",
    "                                    training_data[\"obj_queries\"][p_qid] = [triple]\n",
    "                                else:\n",
    "                                    training_data[\"obj_queries\"][p_qid].append(triple)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "\n",
    "#remove duplicates of triples\n",
    "training_data[\"obj_queries\"] = remove_duplicates(training_data[\"obj_queries\"])\n",
    "\n",
    "with open(trex_triple_file, \"r\") as f:\n",
    "    trex_test = json.load(f)\n",
    "    #remove testdata from training data\n",
    "    count_all_triples = 0\n",
    "    for p_qid in training_data[\"obj_queries\"]:\n",
    "        for triple in training_data[\"obj_queries\"][p_qid]:\n",
    "            count_all_triples = count_all_triples + 1\n",
    "    print(\"Lengths before substraction test from training data {}.\".format(count_all_triples))\n",
    "    training_data[\"obj_queries\"] = substract_triples(training_data[\"obj_queries\"], trex_test)\n",
    "    count_all_triples = 0\n",
    "    for p_qid in training_data[\"obj_queries\"]:\n",
    "        for triple in training_data[\"obj_queries\"][p_qid]:\n",
    "            count_all_triples = count_all_triples + 1\n",
    "    print(\"Lengths after substraction test from training data {}.\".format(count_all_triples))\n",
    "\n",
    "    with open(wikidata_training_path, \"w+\") as f:\n",
    "        json.dump(training_data, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_triple(obj_label, tokenizer):\n",
    "    tokenized_obj = tokenizer.tokenize(obj_label)\n",
    "    if \"[UNK]\" not in tokenized_obj:\n",
    "        if len(tokenized_obj) == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "with open(\"/data/kalo/iswc2020/LAMA/common_vocab_cased.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if check_triple(line, tokenizer) == False:\n",
    "            print(line)\n",
    "        else:\n",
    "            print(line)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of Counter({'P31': 20508383, 'P407': 11317828, 'P17': 5201482, 'P106': 5053504, 'P27': 2221662, 'P1412': 1782630, 'P279': 1077521, 'P19': 828824, 'P131': 780222, 'P20': 412882, 'P495': 347826, 'P413': 312834, 'P136': 231264, 'P364': 228731, 'P937': 175686, 'P1303': 163219, 'P1001': 149718, 'P103': 148823, 'P276': 121847, 'P159': 114081, 'P140': 98585, 'P39': 58426, 'P101': 50890, 'P30': 49295, 'P527': 44526, 'P138': 19536, 'P361': 19473, 'P740': 18941, 'P264': 14681, 'P449': 10660, 'P37': 10621, 'P47': 7392, 'P176': 6186, 'P108': 6041, 'P127': 4884, 'P190': 4749, 'P530': 4195, 'P178': 3223, 'P36': 2500, 'P463': 1757, 'P1376': 337})>\n"
     ]
    }
   ],
   "source": [
    "# counter for properties in wikidata training dataset\n",
    "from collections import Counter\n",
    "c = []\n",
    "with open(wikidata_training_path, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "    for d in training_data:\n",
    "        c.append(d[\"predicate\"])\n",
    "        \n",
    "counter = Counter(c)\n",
    "print(counter.most_common)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
