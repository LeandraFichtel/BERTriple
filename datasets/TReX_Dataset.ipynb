{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of TReX dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a rdf dictionary from wikidata truthy\n",
    "import json\n",
    "\n",
    "entity_dictionary = {}\n",
    "with open(\"/data/wikidata/latest-truthy.nt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"rdf-schema#label\" in line and \"@en \" in line:\n",
    "            try:\n",
    "                s,p ,o = line.split(\"> \")\n",
    "                s = s.replace(\"<\", \"\")\n",
    "                o = o.replace(\"@en\", \"\")\n",
    "                start = o.index( \"\\\"\" ) + 1\n",
    "                end = o.index( \"\\\"\", start )\n",
    "                o = o[start:end]\n",
    "                s = s.encode('utf-8').decode('unicode-escape')\n",
    "                o = o.encode('utf-8').decode('unicode-escape')\n",
    "                    \n",
    "                s = s.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                o = o.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                \n",
    "                entity_dictionary[s] = o\n",
    "            except ValueError:\n",
    "                continue\n",
    "#with open('data/data/entity_dictionary', 'w') as fp:\n",
    "#    json.dump(entity_dictionary, fp, indent=4, sort_keys=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper\n",
    "\n",
    "def remove_duplicates(triples):\n",
    "    seen = set()\n",
    "    new_l = []\n",
    "    for d in triples:\n",
    "        t = tuple(d.items())\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            new_l.append(d)\n",
    "    return new_l\n",
    "\n",
    "def substract_triples(original_triples, substract_triples):\n",
    "    \n",
    "    to_be_removed = set()\n",
    "    for d in substract_triples:\n",
    "        to_be_removed.add(tuple(d.items()))\n",
    "    \n",
    "    new_l = []\n",
    "    for d in original_triples:\n",
    "        t = tuple(d.items())\n",
    "        if t not in to_be_removed:\n",
    "            new_l.append(d)\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 976 triples of property P527.\n",
      "Added 977 triples of property P103.\n",
      "Added 964 triples of property P279.\n",
      "Added 936 triples of property P740.\n",
      "Added 949 triples of property P1303.\n",
      "Added 881 triples of property P449.\n",
      "Added 473 triples of property P140.\n",
      "Added 687 triples of property P127.\n",
      "Added 953 triples of property P20.\n",
      "Added 975 triples of property P30.\n",
      "Added 922 triples of property P31.\n",
      "Added 645 triples of property P138.\n",
      "Added 954 triples of property P937.\n",
      "Added 995 triples of property P190.\n",
      "Added 856 triples of property P364.\n",
      "Added 701 triples of property P1001.\n",
      "Added 892 triples of property P39.\n",
      "Added 958 triples of property P106.\n",
      "Added 234 triples of property P1376.\n",
      "Added 703 triples of property P36.\n",
      "Added 952 triples of property P413.\n",
      "Added 881 triples of property P131.\n",
      "Added 966 triples of property P27.\n",
      "Added 877 triples of property P407.\n",
      "Added 592 triples of property P178.\n",
      "Added 696 triples of property P101.\n",
      "Added 969 triples of property P1412.\n",
      "Added 922 triples of property P47.\n",
      "Added 931 triples of property P136.\n",
      "Added 996 triples of property P530.\n",
      "Added 966 triples of property P37.\n",
      "Added 982 triples of property P176.\n",
      "Added 429 triples of property P264.\n",
      "Added 930 triples of property P17.\n",
      "Added 932 triples of property P361.\n",
      "Added 944 triples of property P19.\n",
      "Added 967 triples of property P159.\n",
      "Added 909 triples of property P495.\n",
      "Added 383 triples of property P108.\n",
      "Added 225 triples of property P463.\n",
      "Added 959 triples of property P276.\n",
      "Lengths before removal 34039.\n",
      "Lengths after removal 34017.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'triple_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-20c33a942171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#print(output_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0ms_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriple_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'triple_dict' is not defined"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "trex_folder = \"data/data/TREx/\"\n",
    "entity_dictionary_file = \"data/data/entity_dictionary\"\n",
    "\n",
    "common_vocab_file = \"./common_vocab_cased.txt\"\n",
    "\n",
    "trex_triple_file = \"data/\"+\"trex_test.json\"\n",
    "output_file = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_label(uri,entity_dictionary):\n",
    "    return entity_dictionary[uri] \n",
    "\n",
    "def load_trex_file(trex_data, entity_dictionary, common_vocab):\n",
    "    global output_file\n",
    "    counter = 0\n",
    "    for triple_dict in trex_data:\n",
    "        #print(triple_dict)\n",
    "        s_qid = triple_dict[\"sub_uri\"]\n",
    "        p_qid = triple_dict[\"predicate_id\"]\n",
    "        o_qid = triple_dict[\"obj_uri\"]\n",
    "        s_label = triple_dict[\"sub_label\"]\n",
    "        o_label = triple_dict[\"obj_label\"]\n",
    "        try:\n",
    "            triple = {\"subject\": s_label, \"predicate\": p_qid, \"object\": o_label}\n",
    "            if o_label in common_vocab:\n",
    "                counter += 1\n",
    "                output_file.append(triple)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print(\"Added {} triples of property {}.\".format(counter, p_qid))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "with open(entity_dictionary_file, \"r\") as f:\n",
    "    entity_dictionary = json.load(f)\n",
    "    with open(common_vocab_file, \"r\") as f:\n",
    "        common_vocab = set()\n",
    "        for line in f:\n",
    "            common_vocab.add(line.strip())\n",
    "        for file in os.listdir(trex_folder):\n",
    "            with open(trex_folder+file, \"r\") as f:\n",
    "                trex_data = []\n",
    "                for line in f.readlines():\n",
    "                    trex_data.append(json.loads(line))\n",
    "                load_trex_file(trex_data, entity_dictionary, common_vocab)\n",
    "\n",
    "        print(\"Lengths before removal {}.\".format(len(output_file)))\n",
    "        output_file = remove_duplicates(output_file)\n",
    "        print(\"Lengths after removal {}.\".format(len(output_file)))\n",
    "        with open(trex_triple_file, \"w\") as f:\n",
    "            json.dump(output_file,f, indent=4, sort_keys=True)\n",
    "#print(output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Wikidata training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =[]\n",
    "properties = {\"P1001\", \"P106\", \"P1303\", \"P1376\", \"P1412\", \"P178\", \"P19\", \"P276\", \"P30\", \"P364\", \"P39\", \"P449\", \"P495\", \"P740\", \"P101\", \"P108\", \"P131\", \"P138\", \"P159\", \"P17\", \"P20\", \"P279\", \"P31\", \"P36\", \"P407\", \"P463\", \"P527\", \"P937\", \"P103\", \"P127\", \"P136\", \"P140\", \"P176\", \"P190\", \"P264\", \"P27\", \"P361\", \"P37\", \"P413\", \"P47\", \"P530\"}\n",
    "wikidata_training_path = \"data/data/TREx/wikidata_training.json\"\n",
    "\n",
    "\n",
    "\n",
    "def get_label(uri,entity_dictionary):\n",
    "    return entity_dictionary[uri] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(entity_dictionary_file, \"r\") as f:\n",
    "    entity_dictionary = json.load(f)\n",
    "    with open(common_vocab_file, \"r\") as f:\n",
    "        common_vocab = set()\n",
    "        for line in f:\n",
    "            common_vocab.add(line.strip())\n",
    "        with open(\"/data/wikidata/latest-truthy.nt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    s,p,o = line.split(\"> <\")\n",
    "                    if \"wikidata.org/entity\" in o and \"wikidata.org/entity\" in s:\n",
    "\n",
    "                        s = s.replace(\"<\", \"\")\n",
    "                        o = o.replace(\"> .\\n\",\"\")\n",
    "                        s = s.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                        o = o.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "                        p = p.replace(\"http://www.wikidata.org/prop/direct/\",\"\")\n",
    "                        p = p.replace(\">\",\"\")\n",
    "                        if p in properties:\n",
    "                            try:\n",
    "                                s = get_label(s, entity_dictionary)\n",
    "                                o = get_label(o, entity_dictionary)\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                            if o in common_vocab:\n",
    "                                triple = {\"subject\": s, \"predicate\": p, \"object\": o}\n",
    "                                training_data.append(triple)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "#remove testdata from training data\n",
    "\n",
    "training_data = remove_duplicates(training_data)\n",
    "training_data = substract_triples(training_data, output_file)\n",
    "\n",
    "#with open(wikidata_training_path, \"w\") as f:\n",
    "#    json.dump(training_data, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_triple(obj_label, tokenizer):\n",
    "    tokenized_obj = tokenizer.tokenize(obj_label)\n",
    "    if \"[UNK]\" not in tokenized_obj:\n",
    "        if len(tokenized_obj) == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "with open(\"/data/kalo/iswc2020/LAMA/common_vocab_cased.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if check_triple(line, tokenizer) == False:\n",
    "            print(line)\n",
    "        else:\n",
    "            print(line)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of Counter({'P31': 20508383, 'P407': 11317828, 'P17': 5201482, 'P106': 5053504, 'P27': 2221662, 'P1412': 1782630, 'P279': 1077521, 'P19': 828824, 'P131': 780222, 'P20': 412882, 'P495': 347826, 'P413': 312834, 'P136': 231264, 'P364': 228731, 'P937': 175686, 'P1303': 163219, 'P1001': 149718, 'P103': 148823, 'P276': 121847, 'P159': 114081, 'P140': 98585, 'P39': 58426, 'P101': 50890, 'P30': 49295, 'P527': 44526, 'P138': 19536, 'P361': 19473, 'P740': 18941, 'P264': 14681, 'P449': 10660, 'P37': 10621, 'P47': 7392, 'P176': 6186, 'P108': 6041, 'P127': 4884, 'P190': 4749, 'P530': 4195, 'P178': 3223, 'P36': 2500, 'P463': 1757, 'P1376': 337})>\n"
     ]
    }
   ],
   "source": [
    "# counter for properties in wikidata training dataset\n",
    "from collections import Counter\n",
    "c = []\n",
    "with open(wikidata_training_path, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "    for d in training_data:\n",
    "        c.append(d[\"predicate\"])\n",
    "        \n",
    "counter = Counter(c)\n",
    "print(counter.most_common)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
